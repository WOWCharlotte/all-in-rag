{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471c46ab",
   "metadata": {},
   "source": [
    "# RAGAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246dddd7",
   "metadata": {},
   "source": [
    "> [RAGAS](https://docs.ragas.io/en/stable/getstarted/) 是一个用于评估 RAG 系统的框架，它提供了一系列的工具，可以显著提升LLM应用的评估效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cdcf12",
   "metadata": {},
   "source": [
    "## 为什么需要评估RAG？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f394b98",
   "metadata": {},
   "source": [
    "RAG系统从外部知识库中检索信息，组装上下文，再回答用户问题，不仅降低了LLM的“幻觉”问题，还提升的答案的质量。但如果我们想让RAG生成答案的质量再高一点，就会遇到一个问题，我怎么改进RAG系统，哪些改进可以使RAG生成的答案更准确呢？RAG系统包含：提示词、分块、索引、嵌入、查询、检索、生成等众多组件/环节，哪个环节的改动才能使RAG生成答案的质量更高呢？这需要一个评估标准来帮助我们定量分析。RAGAS就是这样一个评估框架用于客观评估RAG的生成质量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca07d5b",
   "metadata": {},
   "source": [
    "## 本章内容"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c977d915",
   "metadata": {},
   "source": [
    "你将在本章学习RAGAS的基础知识并构建一个简单的RAG评估管道，实现从测试集生成->RAG评估的完整流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b6b261",
   "metadata": {},
   "source": [
    "## 项目结构\n",
    "本章的文件结构如下：\n",
    "```\n",
    "rag_eval/\n",
    "|-- evals\n",
    "|   |-- datasets # 数据集\n",
    "|   |-- experiments # 测试结果\n",
    "|   |-- logs # 测试日志 \n",
    "|-- env.example # 配置文件示例\n",
    "|-- evals.py # 评估脚本\n",
    "|-- gen_pipline.py # 测试用例生成脚本\n",
    "|-- rag.py # 简单的RAG系统\n",
    "|-- pyproject.toml # 项目配置文件\n",
    "|-- ragas_demo.ipynb # 教程\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9330fefb",
   "metadata": {},
   "source": [
    "## 安装依赖&配置环境变量\n",
    "```\n",
    "cd rag_eval\n",
    "uv sync\n",
    "cp .env.example .env\n",
    "```\n",
    "填写 `.env` 文件中的LLM API KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49c2e1",
   "metadata": {},
   "source": [
    "## 构建测试集用于RAG评估\n",
    "构建测试集是一个非常困难的步骤，因为不同行业面对的数据格式、规模、结构都不同，关注的点也不同，这导致难以评估测试集的好坏。本节使用的是相对简单的数据集用于演示，实际应用中需要根据实际情况构建测试集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c0d7c8",
   "metadata": {},
   "source": [
    "1. 数据集介绍<br>\n",
    "本章教学使用`all-in-rag`的第一章教程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9051b8",
   "metadata": {},
   "source": [
    "2. 加载数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f45dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "path = \"./evals/datasets/custom\"\n",
    "loader = DirectoryLoader(path, glob=\"**/*.md\")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7201ea69",
   "metadata": {},
   "source": [
    "3. 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64ead98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\hello-agent\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded env\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\206085754.py:23: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  llm = LangchainLLMWrapper(llm,cache=cache)\n",
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\206085754.py:29: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  embeddings = LangchainEmbeddingsWrapper(embeddings,cache=cache)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ragas import DiskCacheBackend\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "if load_dotenv(\".env\"):\n",
    "    print(\"Loaded env\")\n",
    "    ENV_LOADED = True\n",
    "else:\n",
    "    print(\"No env loaded\")\n",
    "    ENV_LOADED = False\n",
    "\n",
    "if ENV_LOADED:\n",
    "    cache = DiskCacheBackend()\n",
    "    llm = ChatOpenAI(\n",
    "        model=os.getenv(\"EVAL_LLM_MODEL\"),\n",
    "        base_url=os.getenv(\"EVAL_LLM_BINDING_HOST\"),\n",
    "        api_key=os.getenv(\"EVAL_LLM_BINDING_API_KEY\"),\n",
    "        temperature=1.0,\n",
    "    )\n",
    "    llm = LangchainLLMWrapper(llm,cache=cache)\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model = os.getenv(\"EVAL_EMBEDDING_MODEL\"),\n",
    "        base_url=os.getenv(\"EVAL_EMBEDDING_BINDING_HOST\"),\n",
    "        api_key=os.getenv(\"EVAL_EMBEDDING_BINDING_API_KEY\")\n",
    "    )\n",
    "    embeddings = LangchainEmbeddingsWrapper(embeddings,cache=cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b44e1f",
   "metadata": {},
   "source": [
    "4. 设置不同角色\n",
    "- RAGAS支持使用`Persona`类定义不同的角色，不同角色对生成答案的关注点不同，这有助于扩大测试用例的覆盖面。\n",
    "    ```\n",
    "    class Persona(BaseModel):\n",
    "        name: str\n",
    "        role_description: str\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15b6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.persona import Persona\n",
    "\n",
    "# 设定角色\n",
    "personas = [\n",
    "    Persona(\n",
    "        name=\"大学生\",\n",
    "        role_description=\"在校大学生，没有AI经验，想要入门RAG\",\n",
    "    ),\n",
    "    Persona(\n",
    "        name=\"AI应用工程师\",\n",
    "        role_description=\"AI应用工程师，有三年工作经验，想要了解RAG的技术原理\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aef007",
   "metadata": {},
   "source": [
    "5. 初始化测试用例生成器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820d3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(\n",
    "    llm=llm, embedding_model=embeddings, persona_list=personas\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b08083",
   "metadata": {},
   "source": [
    "6. 配置查询类型并适应目标语言<br>\n",
    "在这里，我们加载所需的查询类型，并将其适配到目标语言。\n",
    "- `SingleHopSpecificQuerySynthesizer`类是一个单跳问答生成策略，单跳问答是指对于一个简单的事实类问题,可以将知识图谱中存在的与问题实体直接相连的独立3元组作为答案。\n",
    "- `MultiHopSpecificQuerySynthesizer`类是一个多跳问答生成策略，多跳问答指问答系统需要在知识图谱中的多个相关联的3元组之间执行推理才能获得正确答案."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee25686",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.testset.synthesizers.single_hop.specific import (\n",
    "    SingleHopSpecificQuerySynthesizer,\n",
    ")\n",
    "from ragas.testset.synthesizers.multi_hop.specific import MultiHopSpecificQuerySynthesizer\n",
    "\n",
    "distribution = [\n",
    "    (SingleHopSpecificQuerySynthesizer(llm=llm), 0.5),\n",
    "    (MultiHopSpecificQuerySynthesizer(llm=llm), 0.5),\n",
    "]\n",
    "\n",
    "# 使生成的测试用例为中文。其原理是在LLM的提示词中加入指定的目标语言类型，利用LLM输出，但实际上依旧使用英文输出\n",
    "for query, _ in distribution:\n",
    "    prompts = await query.adapt_prompts(\"chinese\", llm=llm)\n",
    "    query.set_prompts(**prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb009f54",
   "metadata": {},
   "source": [
    "7. 生成并保存测试用例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b207ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = generator.generate_with_langchain_docs(\n",
    "    docs[:],\n",
    "    testset_size=10, # 生成的测试用例数量\n",
    "    query_distribution=distribution,\n",
    ")\n",
    "dataset = dataset.to_pandas()\n",
    "dataset.to_csv(\n",
    "    \"./evals/datasets/testcase.csv\",\n",
    "    index=False,\n",
    "    encoding='utf-8'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973aba1d",
   "metadata": {},
   "source": [
    "9. 创建`GeneratePipeline`类<br>\n",
    "为了实现可复用的测试用例生成管道，我们创建一个`GeneratePipeline`类，可执行批量的测试用例生成任务，完整代码详见`gen_pipeline.py`文件。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf949b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.cache import DiskCacheBackend\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.testset import TestsetGenerator\n",
    "from ragas.testset.persona import Persona\n",
    "from ragas.testset.synthesizers.single_hop.specific import SingleHopSpecificQuerySynthesizer\n",
    "from ragas.testset.synthesizers.multi_hop.specific import MultiHopSpecificQuerySynthesizer\n",
    "import traceback\n",
    "import logging\n",
    " \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "if load_dotenv(\".env\"):\n",
    "    print(\"Loaded environment variables from .env file\")\n",
    "    ENV_LOADED = True\n",
    "else:\n",
    "    print(\"No .env file found\")\n",
    "    ENV_LOADED = False\n",
    "\n",
    "# 设置日志级别：INFO\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "class GeneratePipeline():\n",
    "    def __init__(self,input_dir:str,output_dir:str='evals/experiments'):\n",
    "        if ENV_LOADED:\n",
    "            cache = DiskCacheBackend()\n",
    "            llm = ChatOpenAI(\n",
    "                model=os.getenv(\"EVAL_LLM_MODEL\"),\n",
    "                base_url=os.getenv(\"EVAL_LLM_BINDING_HOST\"),\n",
    "                api_key=os.getenv(\"EVAL_LLM_BINDING_API_KEY\"),\n",
    "                temperature=1.0,\n",
    "            )\n",
    "            embeddings = OpenAIEmbeddings(\n",
    "                model = os.getenv(\"EVAL_EMBEDDING_MODEL\"),\n",
    "                base_url=os.getenv(\"EVAL_EMBEDDING_BINDING_HOST\"),\n",
    "                api_key=os.getenv(\"EVAL_EMBEDDING_BINDING_API_KEY\")\n",
    "            )\n",
    "            self.llm = LangchainLLMWrapper(llm, cache=cache)\n",
    "            self.embeddings = LangchainEmbeddingsWrapper(embeddings, cache=cache)\n",
    "            self.input_dir = input_dir\n",
    "            if os.path.exists(output_dir):\n",
    "                self.output_dir = output_dir\n",
    "            else:\n",
    "                os.makedirs(output_dir)\n",
    "                self.output_dir = output_dir\n",
    "            self._display_configuration()\n",
    "        else:\n",
    "            raise Exception(\"Please load environment variables from .env file\")\n",
    "        \n",
    "    def _display_configuration(self):\n",
    "        # 打印模型配置信息\n",
    "        logger.info(\"Model Configuration:\")\n",
    "        logger.info(f\"LLM: {self.llm}\")\n",
    "        logger.info(f\"Embeddings: {self.embeddings}\")\n",
    "        # 打印文件配置信息\n",
    "        logger.info(\"File Configuration:\")\n",
    "        logger.info(f\"Input Directory: {self.input_dir}\")\n",
    "        logger.info(f\"Output Directory: {self.output_dir}\")\n",
    "\n",
    "    async def _load_dataset(self,file_type:str='txt'):\n",
    "        loader = DirectoryLoader(self.input_dir, glob=f\"**/*.{file_type}\") # 实际应用中数据格式多样，需要不同的数据加载策略\n",
    "        docs = loader.load()\n",
    "        logger.info(f\"Loaded {len(docs)} documents\")\n",
    "        return docs\n",
    "    \n",
    "    async def _gen_rag_testcase(\n",
    "            self,\n",
    "            docs,\n",
    "            single_rate:float=0.5,\n",
    "            multi_rate:float=0.5,\n",
    "            language:str='chinese',\n",
    "    ):\n",
    "        # 安全判断\n",
    "        if not os.path.exists(self.input_dir):\n",
    "            raise Exception(f\"Input directory {self.input_dir} does not exist\")\n",
    "        if single_rate < 0.0 or single_rate > 1.0 or multi_rate < 0.0 or multi_rate > 1.0:\n",
    "            raise Exception(\"Rate must be between 0.0 and 1.0\")\n",
    "        if single_rate + multi_rate != 1.0:\n",
    "            raise Exception(\"Single rate and multi rate must sum up to 1.0\")\n",
    "        if not docs:\n",
    "            raise Exception(\"No documents loaded. Please run _load_dataset() before generating test cases\")\n",
    "        # 设定角色\n",
    "        personas = [\n",
    "            Persona(\n",
    "                name=\"大学生\",\n",
    "                role_description=\"在校大学生，没有AI经验，想要入门RAG\",\n",
    "            ),\n",
    "            Persona(\n",
    "                name=\"AI应用工程师\",\n",
    "                role_description=\"AI应用工程师，有三年工作经验，想要了解RAG的技术原理\",\n",
    "            )\n",
    "        ]\n",
    "        # 单跳/多跳查询问题比例设置\n",
    "        if single_rate == 1.0:\n",
    "            distribution = [\n",
    "                (SingleHopSpecificQuerySynthesizer(llm=self.llm), 1.0),\n",
    "            ]\n",
    "        elif multi_rate == 1.0:\n",
    "            distribution = [\n",
    "                (MultiHopSpecificQuerySynthesizer(llm=self.llm), 1.0),\n",
    "            ]\n",
    "        else:\n",
    "            distribution = [\n",
    "                (SingleHopSpecificQuerySynthesizer(llm=self.llm),single_rate),\n",
    "                (MultiHopSpecificQuerySynthesizer(llm=self.llm), multi_rate),\n",
    "            ]\n",
    "        for query, _ in distribution:\n",
    "            # 适配中文提示词，提升生成任务的中文场景贴合度\n",
    "            prompts = await query.adapt_prompts(language, llm=self.llm)\n",
    "            query.set_prompts(**prompts)\n",
    "\n",
    "        generator = TestsetGenerator(\n",
    "            llm=self.llm,\n",
    "            embedding_model=self.embeddings,\n",
    "            persona_list=personas\n",
    "        )\n",
    "        try:\n",
    "            dataset = generator.generate_with_langchain_docs(\n",
    "                docs[:],\n",
    "                testset_size=10, # 生成的测试用例数量\n",
    "                query_distribution=distribution,\n",
    "            )\n",
    "            dataset = dataset.to_pandas()\n",
    "            dataset.to_csv(\n",
    "                f\"{self.output_dir}/testcase.csv\",\n",
    "                index=False,\n",
    "                encoding=\"utf-8\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating test cases: {e}\")\n",
    "            logger.error(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c47f8",
   "metadata": {},
   "source": [
    "## 构建RAG评估系统"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28373dd",
   "metadata": {},
   "source": [
    "### 常见的RAG评估指标"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c57f4c4",
   "metadata": {},
   "source": [
    "#### 上下文精确度\n",
    "\n",
    ">上下文精确度用于评估检索器在给定查询的检索上下文中，将相关数据块的排名高于不相关数据块的能力。具体来说，它评估的是检索上下文中的相关数据块在排名中处于顶部的程度，这类似于`重排序`的功能，将高语义相关的数据块排在顶部，低语义相关的数据块排在底部。它是通过计算上下文中每个数据块的 precision@k 的平均值得得到的。Precision@k 是排名为 k 的相关数据块的数量与排名为 k 的数据块总数的比值。\n",
    "\n",
    "- 公式\n",
    "$$\n",
    "\\text{Context Precision@}K = \\frac{\\sum_{k=1}^{K} \\left( \\text{Precision@}k \\times v_k \\right)}{\\text{Total number of relevant items in the top } K \\text{ results}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Precision@}k = \\frac{\\text{true positives@}k}{(\\text{true positives@}k + \\text{false positives@}k)}\n",
    "$$\n",
    "其中，$K$ 是块的总数，`retrieved_contexts` 并且 $v_k \\in \\{0,1\\}$ 是排名相关性指标 $k$。<br>\n",
    "下面我们通过一个示例来了解上下文精确度的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b98934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\3743511296.py:2: DeprecationWarning: Importing ContextPrecision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import ContextPrecision\n",
      "  from ragas.metrics import ContextPrecision\n",
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\3743511296.py:12: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  llm = LangchainLLMWrapper(llm)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1/1 [00:11<00:00, 11.80s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Where is the Eiffel Tower located?</td>\n",
       "      <td>[The Eiffel Tower is located in Paris., The Br...</td>\n",
       "      <td>The Eiffel Tower is located in Paris.</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           user_input  \\\n",
       "0  Where is the Eiffel Tower located?   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [The Eiffel Tower is located in Paris., The Br...   \n",
       "\n",
       "                               reference  context_precision  \n",
       "0  The Eiffel Tower is located in Paris.                1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import ContextPrecision\n",
    "from datasets import Dataset\n",
    "\n",
    "# Setup LLM\n",
    "llm = ChatOpenAI( \n",
    "    model=os.getenv(\"EVAL_LLM_MODEL\"),\n",
    "    base_url=os.getenv(\"EVAL_LLM_BINDING_HOST\"),\n",
    "    api_key=os.getenv(\"EVAL_LLM_BINDING_API_KEY\"),\n",
    "    temperature=1.0,\n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "# Create metric\n",
    "metrics = [ContextPrecision()]\n",
    "\n",
    "# Dataset\n",
    "eval_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": [\"Where is the Eiffel Tower located?\"],\n",
    "        \"contexts\": [[\"The Eiffel Tower is located in Paris.\", \"The Brandenburg Gate is located in Berlin.\"]],\n",
    "        \"ground_truth\": [\"The Eiffel Tower is located in Paris.\"],\n",
    "    }\n",
    ")\n",
    "# Evaluate\n",
    "eval_results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "df = eval_results.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0aeec",
   "metadata": {},
   "source": [
    "#### 上下文召回率（Context Recall）\n",
    "> 上下文召回率衡量的是成功检索到的相关文档（或信息片段）的数量。它侧重于不遗漏重要结果。召回率越高，意味着被遗漏的相关文档越少。简而言之，召回率关乎不遗漏任何重要内容。\n",
    "- 公式\n",
    "$$\n",
    "\\text{Context Recall} = \\frac{\\text{检索到的相关文档数量}}{\\text{相关文档的总数}}\n",
    "$$\n",
    "下面通过一个示例来了解上下文召回率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14c9f751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\1233587777.py:2: DeprecationWarning: Importing ContextRecall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import ContextRecall\n",
      "  from ragas.metrics import ContextRecall\n",
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\1233587777.py:12: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  llm = LangchainLLMWrapper(llm)\n",
      "Evaluating: 100%|██████████| 1/1 [00:07<00:00,  7.47s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was the first super bowl?</td>\n",
       "      <td>[The First AFL–NFL World Championship Game was...</td>\n",
       "      <td>The first superbowl was held on Jan 15, 1967</td>\n",
       "      <td>The first superbowl was held on Jan 15, 1967</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_input  \\\n",
       "0  When was the first super bowl?   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [The First AFL–NFL World Championship Game was...   \n",
       "\n",
       "                                       response  \\\n",
       "0  The first superbowl was held on Jan 15, 1967   \n",
       "\n",
       "                                      reference  context_recall  \n",
       "0  The first superbowl was held on Jan 15, 1967             1.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import ContextRecall\n",
    "from datasets import Dataset\n",
    "\n",
    "# Setup LLM\n",
    "llm = ChatOpenAI( \n",
    "    model=os.getenv(\"EVAL_LLM_MODEL\"),\n",
    "    base_url=os.getenv(\"EVAL_LLM_BINDING_HOST\"),\n",
    "    api_key=os.getenv(\"EVAL_LLM_BINDING_API_KEY\"),\n",
    "    temperature=1.0,\n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "# Create metric\n",
    "metrics = [ContextRecall()]\n",
    "\n",
    "# Dataset\n",
    "eval_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": [\"When was the first super bowl?\"],  \n",
    "        \"answer\": [\"The first superbowl was held on Jan 15, 1967\"],\n",
    "        \"retrieved_contexts\":[[\"The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\"]],\n",
    "        \"ground_truth\":[\"The first superbowl was held on Jan 15, 1967\"]\n",
    "    }\n",
    ")\n",
    "# Evaluate\n",
    "eval_results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "df = eval_results.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248b2748",
   "metadata": {},
   "source": [
    "#### 答案相关性指标\n",
    "> “答案相关性”指标用于衡量RAG响应与用于查询的相关程度，其取值范围为0到1，分数越高表示与用户查询的匹配度越好。若一个答案能够直接且恰当地回应原始问题，则被认为是相关的。该指标聚焦于答案与问题意图的匹配程度，**不评估事实准确性**；同时会对不完整或包含多余细节的答案进行扣分。\n",
    "\n",
    "\n",
    "-计算方式\n",
    "该指标通过`user_input`（用户输入）和`response`（响应）计算，步骤如下：\n",
    "1. 基于响应生成一组人工问题（默认数量为3），这些问题需体现响应的内容。\n",
    "2. 计算用户输入的嵌入向量（\\(E_o\\)）与每个生成问题的嵌入向量（\\(E_{g_i}\\)）之间的余弦相似度。\n",
    "3. 对这些余弦相似度分数取平均值，得到“答案相关性”。\n",
    "\n",
    "\n",
    "- 公式\n",
    "\\[\n",
    "\\text{Answer Relevancy} = \\frac{1}{N} \\sum_{i=1}^{N} \\text{cosine similarity}(E_{g_i}, E_o)\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\text{Answer Relevancy} = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{E_{g_i} \\cdot E_o}{\\|E_{g_i}\\| \\|E_o\\|}\n",
    "\\]\n",
    "\n",
    "\n",
    "- 符号说明\n",
    "- \\(E_{g_i}\\)：第\\(i\\)个生成问题的嵌入向量。\n",
    "- \\(E_o\\)：用户输入的嵌入向量。\n",
    "- \\(N\\)：生成问题的数量（默认值为3，可通过`strictness`参数配置）。<br>\n",
    "\n",
    "下面通过一个示例来了解答案相关性指标的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257a3b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\773549811.py:2: DeprecationWarning: Importing AnswerRelevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import AnswerRelevancy\n",
      "  from ragas.metrics import AnswerRelevancy\n",
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\773549811.py:12: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  llm = LangchainLLMWrapper(llm)\n",
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\773549811.py:19: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  embeddings = LangchainEmbeddingsWrapper(embeddings,cache=cache)\n",
      "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]Exception raised in Job[0]: BadRequestError(Error code: 400 - {'error': {'message': 'Invalid n value (currently only n = 1 is supported)', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}})\n",
      "Evaluating: 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>response</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was the first super bowl?</td>\n",
       "      <td>The first superbowl was held on Jan 15, 1967</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_input  \\\n",
       "0  When was the first super bowl?   \n",
       "\n",
       "                                       response  answer_relevancy  \n",
       "0  The first superbowl was held on Jan 15, 1967               NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import AnswerRelevancy\n",
    "from datasets import Dataset\n",
    "\n",
    "# Setup LLM\n",
    "llm = ChatOpenAI( \n",
    "    model=os.getenv(\"EVAL_LLM_MODEL\"),\n",
    "    base_url=os.getenv(\"EVAL_LLM_BINDING_HOST\"),\n",
    "    api_key=os.getenv(\"EVAL_LLM_BINDING_API_KEY\"),\n",
    "    temperature=1.0,\n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "        model = os.getenv(\"EVAL_EMBEDDING_MODEL\"),\n",
    "        base_url=os.getenv(\"EVAL_EMBEDDING_BINDING_HOST\"),\n",
    "        api_key=os.getenv(\"EVAL_EMBEDDING_BINDING_API_KEY\")\n",
    "    )\n",
    "embeddings = LangchainEmbeddingsWrapper(embeddings,cache=cache)\n",
    "\n",
    "# Create metric\n",
    "metrics = [AnswerRelevancy()]\n",
    "\n",
    "# Dataset\n",
    "eval_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": [\"When was the first super bowl?\"],  \n",
    "        \"answer\": [\"The first superbowl was held on Jan 15, 1967\"],\n",
    "        \"retrieved_contexts\":[[\"The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\"]],\n",
    "        \"ground_truth\":[\"The first superbowl was held on Jan 15, 1967\"]\n",
    "    }\n",
    ")\n",
    "# Evaluate\n",
    "eval_results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "\n",
    "df = eval_results.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373cea55",
   "metadata": {},
   "source": [
    "#### 忠实度\n",
    "> 忠实度指标用于衡量响应与检索到的上下文在事实层面的一致性。其取值范围为0到1，分数越高表明一致性越好。简单来说忠诚度用于衡量响应是否遵循检索到的上下文。\n",
    "- 忠诚度和答案相关性的区别在于:如果检索到的上下文中没有包含能够回答检索问题的答案，RAG响应为“抱歉，基于现有上下文我无法回答您的问题”，此时的忠诚度等于1，而答案相关性等于0，因为响应完全是基于检索到的上下文来回答的，即便它无法准确回答检索的问题。\n",
    "\n",
    "- 计算方法\n",
    "1. 识别响应中的所有答案。\n",
    "2. 检查每个答案是否能从检索到的上下文中推断出来。\n",
    "3. 使用以下公式计算忠实度分数：\n",
    "$$\n",
    "\\text{忠诚度} = \\frac{\\text{响应中由检索到的上下文支持的答案数量}}{\\text{检索到的上下文总数量}}\n",
    "$$\n",
    "下面通过一个示例了解忠实度的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "125dcee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\755493608.py:2: DeprecationWarning: Importing Faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import Faithfulness\n",
      "  from ragas.metrics import Faithfulness\n",
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\755493608.py:12: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  llm = LangchainLLMWrapper(llm)\n",
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\755493608.py:19: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  embeddings = LangchainEmbeddingsWrapper(embeddings,cache=cache)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 1/1 [00:12<00:00, 12.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was the first super bowl?</td>\n",
       "      <td>[The First AFL–NFL World Championship Game was...</td>\n",
       "      <td>The first superbowl was held on Jan 15, 1967</td>\n",
       "      <td>The first superbowl was held on Jan 15, 1967</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_input  \\\n",
       "0  When was the first super bowl?   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [The First AFL–NFL World Championship Game was...   \n",
       "\n",
       "                                       response  \\\n",
       "0  The first superbowl was held on Jan 15, 1967   \n",
       "\n",
       "                                      reference  faithfulness  \n",
       "0  The first superbowl was held on Jan 15, 1967           1.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import Faithfulness\n",
    "from datasets import Dataset\n",
    "\n",
    "# Setup LLM\n",
    "llm = ChatOpenAI( \n",
    "    model=os.getenv(\"EVAL_LLM_MODEL\"),\n",
    "    base_url=os.getenv(\"EVAL_LLM_BINDING_HOST\"),\n",
    "    api_key=os.getenv(\"EVAL_LLM_BINDING_API_KEY\"),\n",
    "    temperature=1.0,\n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "        model = os.getenv(\"EVAL_EMBEDDING_MODEL\"),\n",
    "        base_url=os.getenv(\"EVAL_EMBEDDING_BINDING_HOST\"),\n",
    "        api_key=os.getenv(\"EVAL_EMBEDDING_BINDING_API_KEY\")\n",
    "    )\n",
    "embeddings = LangchainEmbeddingsWrapper(embeddings,cache=cache)\n",
    "\n",
    "# Create metric\n",
    "metrics = [Faithfulness()]\n",
    "\n",
    "# Dataset\n",
    "eval_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": [\"When was the first super bowl?\"],  \n",
    "        \"answer\": [\"The first superbowl was held on Jan 15, 1967\"],\n",
    "        \"retrieved_contexts\":[[\"The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\"]],\n",
    "        \"ground_truth\":[\"The first superbowl was held on Jan 15, 1967\"]\n",
    "    }\n",
    ")\n",
    "# Evaluate\n",
    "eval_results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "\n",
    "df = eval_results.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e966793",
   "metadata": {},
   "source": [
    "#### 噪声敏感度\n",
    "> 噪声敏感度用于衡量系统在使用相关或不相关的检索文档时给出错误响应的频率。该分数范围为0到1，数值越低表示性能越好。噪声敏感度是通过用户输入、参考信息、响应以及检索到的上下文来计算的。\n",
    "- 为了评估噪声敏感性，需要对生成的回答中的每个答案进行检查，以确定其是否基于真实情况正确无误，以及是否可归因于相关（或不相关）的检索到的上下文。理想情况下，回答中的所有答案都应得到相关检索到的上下文的支持。\n",
    "- 计算公式\n",
    "\n",
    "$$\\text{噪声敏感性（相关）} = \\frac{|\\text{回复中错误答案的总数}|}{|\\text{回复中的答案总数}|}$$\n",
    "下面通过一个示例来了解噪声敏感度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "553f5243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\397696283.py:2: DeprecationWarning: Importing NoiseSensitivity from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import NoiseSensitivity\n",
      "  from ragas.metrics import NoiseSensitivity\n",
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\397696283.py:12: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  llm = LangchainLLMWrapper(llm)\n",
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\397696283.py:19: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  embeddings = LangchainEmbeddingsWrapper(embeddings,cache=cache)\n",
      "Evaluating: 100%|██████████| 1/1 [00:30<00:00, 30.46s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>noise_sensitivity(mode=relevant)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was the first super bowl?</td>\n",
       "      <td>[The First AFL–NFL World Championship Game was...</td>\n",
       "      <td>The first superbowl was held on Jan 15, 1967</td>\n",
       "      <td>The first superbowl was held on Jan 15, 1967</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_input  \\\n",
       "0  When was the first super bowl?   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [The First AFL–NFL World Championship Game was...   \n",
       "\n",
       "                                       response  \\\n",
       "0  The first superbowl was held on Jan 15, 1967   \n",
       "\n",
       "                                      reference  \\\n",
       "0  The first superbowl was held on Jan 15, 1967   \n",
       "\n",
       "   noise_sensitivity(mode=relevant)  \n",
       "0                               0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import NoiseSensitivity\n",
    "from datasets import Dataset\n",
    "\n",
    "# Setup LLM\n",
    "llm = ChatOpenAI( \n",
    "    model=os.getenv(\"EVAL_LLM_MODEL\"),\n",
    "    base_url=os.getenv(\"EVAL_LLM_BINDING_HOST\"),\n",
    "    api_key=os.getenv(\"EVAL_LLM_BINDING_API_KEY\"),\n",
    "    temperature=1.0,\n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "        model = os.getenv(\"EVAL_EMBEDDING_MODEL\"),\n",
    "        base_url=os.getenv(\"EVAL_EMBEDDING_BINDING_HOST\"),\n",
    "        api_key=os.getenv(\"EVAL_EMBEDDING_BINDING_API_KEY\")\n",
    "    )\n",
    "embeddings = LangchainEmbeddingsWrapper(embeddings,cache=cache)\n",
    "\n",
    "# Create metric\n",
    "metrics = [NoiseSensitivity()]\n",
    "\n",
    "# Dataset\n",
    "eval_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": [\"When was the first super bowl?\"],  \n",
    "        \"answer\": [\"The first superbowl was held on Jan 15, 1967\"],\n",
    "        \"retrieved_contexts\":[[\"The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\"]],\n",
    "        \"ground_truth\":[\"The first superbowl was held on Jan 15, 1967\"]\n",
    "    }\n",
    ")\n",
    "# Evaluate\n",
    "eval_results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "\n",
    "df = eval_results.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd524b0",
   "metadata": {},
   "source": [
    "#### 上下文实体召回率\n",
    "\n",
    "> 上下文实体召回率（Context Entity Recall） 指标用于衡量检索到的上下文的召回程度，其计算基于同时存在于“参考文本”和“检索到的上下文”中的实体数量，与仅存在于“参考文本”中的实体数量的相对比例。简单来说，它衡量的是从参考文本中召回的实体占比。\n",
    "\n",
    "- 该指标适用于基于事实的场景（例如旅游服务台、历史问答等），可帮助评估实体检索机制——因为在实体很重要的场景中，我们需要检索到的上下文能覆盖这些实体。该指标主要用于融合`知识图谱`的高级RAG系统，例如：`GraphRAG``LightRAG``RAGFlow``KAG`等。\n",
    "\n",
    "\n",
    "- 计算该指标需用到两个集合：\n",
    "    - \\( RE \\)：参考文本中的实体集合。\n",
    "    - \\( RCE \\)：检索到的上下文中的实体集合。\n",
    "\n",
    "\n",
    "我们先计算两个集合的公共实体数量（ RCE和RE ），再将其除以参考文本中的实体总数（RE）。公式为：\n",
    "$$\\text{上下文实体召回率} = \\frac{\\text{RE与RCE的公共实体数量}}{\\text{RE中的实体总数}}$$\n",
    "下面我们通过一个示例来了解上下文实体召回率的使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc634e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\177973621.py:2: DeprecationWarning: Importing ContextEntityRecall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import ContextEntityRecall\n",
      "  from ragas.metrics import ContextEntityRecall\n",
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\177973621.py:12: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  llm = LangchainLLMWrapper(llm)\n",
      "C:\\Users\\charlotte\\AppData\\Local\\Temp\\ipykernel_59100\\177973621.py:19: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  embeddings = LangchainEmbeddingsWrapper(embeddings,cache=cache)\n",
      "Evaluating: 100%|██████████| 1/1 [00:10<00:00, 10.20s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_entity_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When was the first super bowl?</td>\n",
       "      <td>[The First AFL–NFL World Championship Game was...</td>\n",
       "      <td>The first superbowl was held on Jan 15, 1967</td>\n",
       "      <td>The first superbowl was held on Jan 15, 1967</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_input  \\\n",
       "0  When was the first super bowl?   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [The First AFL–NFL World Championship Game was...   \n",
       "\n",
       "                                       response  \\\n",
       "0  The first superbowl was held on Jan 15, 1967   \n",
       "\n",
       "                                      reference  context_entity_recall  \n",
       "0  The first superbowl was held on Jan 15, 1967                    0.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import ContextEntityRecall\n",
    "from datasets import Dataset\n",
    "\n",
    "# Setup LLM\n",
    "llm = ChatOpenAI( \n",
    "    model=os.getenv(\"EVAL_LLM_MODEL\"),\n",
    "    base_url=os.getenv(\"EVAL_LLM_BINDING_HOST\"),\n",
    "    api_key=os.getenv(\"EVAL_LLM_BINDING_API_KEY\"),\n",
    "    temperature=1.0,\n",
    ")\n",
    "llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "        model = os.getenv(\"EVAL_EMBEDDING_MODEL\"),\n",
    "        base_url=os.getenv(\"EVAL_EMBEDDING_BINDING_HOST\"),\n",
    "        api_key=os.getenv(\"EVAL_EMBEDDING_BINDING_API_KEY\")\n",
    "    )\n",
    "embeddings = LangchainEmbeddingsWrapper(embeddings,cache=cache)\n",
    "\n",
    "# Create metric\n",
    "metrics = [ContextEntityRecall()]\n",
    "\n",
    "# Dataset\n",
    "eval_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"question\": [\"When was the first super bowl?\"],  \n",
    "        \"answer\": [\"The first superbowl was held on Jan 15, 1967\"],\n",
    "        \"retrieved_contexts\":[[\"The First AFL–NFL World Championship Game was an American football game played on January 15, 1967, at the Los Angeles Memorial Coliseum in Los Angeles.\"]],\n",
    "        \"ground_truth\":[\"The first superbowl was held on Jan 15, 1967\"]\n",
    "    }\n",
    ")\n",
    "# Evaluate\n",
    "eval_results = evaluate(\n",
    "    dataset=eval_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=llm,\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "\n",
    "df = eval_results.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d43e5",
   "metadata": {},
   "source": [
    "### 自动化的RAG评估管道\n",
    "我们将在本小节实现一个通用的支持多并发的RAG评估管道，完整代码详见`evals_pipeline.py`，它依赖于已经构建好的简易RAG系统`rag.py`。由于本章的重点在于如何评估RAG效果，因此关于这个简易RAG系统实现的细节不展开描述。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d6cd8e",
   "metadata": {},
   "source": [
    "#### 类结构设计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f73e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "class RAGEvalPipeline:\n",
    "    def __init__(self,  test_dataset_path: str = \"evals/experiments/testcase.csv\"):\n",
    "        if ENV_LOADED:\n",
    "            # 初始化模型\n",
    "            cache = DiskCacheBackend()\n",
    "            llm = ChatOpenAI( \n",
    "                model=os.getenv(\"EVAL_LLM_MODEL\"),\n",
    "                base_url=os.getenv(\"EVAL_LLM_BINDING_HOST\"),\n",
    "                api_key=os.getenv(\"EVAL_LLM_BINDING_API_KEY\"),\n",
    "                temperature=1.0,\n",
    "            )\n",
    "            self.chat_model = llm\n",
    "            self.llm = LangchainLLMWrapper(llm, cache=cache)\n",
    "            embeddings = OpenAIEmbeddings(\n",
    "                model = os.getenv(\"EVAL_EMBEDDING_MODEL\"),\n",
    "                base_url=os.getenv(\"EVAL_EMBEDDING_BINDING_HOST\"),\n",
    "                api_key=os.getenv(\"EVAL_EMBEDDING_BINDING_API_KEY\")\n",
    "            )\n",
    "            self.embeddings = LangchainEmbeddingsWrapper(embeddings, cache=cache)\n",
    "            self.eval_max_retries = int(os.getenv(\"EVAL_MAX_RETRIES\", \"5\"))\n",
    "            self.eval_timeout = int(os.getenv(\"EVAL_TIMEOUT\", \"60\"))\n",
    "            self.test_dataset_path = Path(test_dataset_path)\n",
    "            self.results_dir = Path(__file__).parent / \"results\"\n",
    "            self.results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            # Add FileHandler\n",
    "            log_file = self.results_dir / \"eval_pipeline.log\"\n",
    "            fh = logging.FileHandler(log_file, encoding='utf-8')\n",
    "            fh.setLevel(logging.INFO)\n",
    "            fh.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))\n",
    "            logger.addHandler(fh)\n",
    "\n",
    "            self.test_cases = self._load_test_dataset()\n",
    "            self._display_configuration()\n",
    "        else:\n",
    "            raise Exception(\"Please load environment variables from .env file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7bdebf",
   "metadata": {},
   "source": [
    "#### 加载测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc40f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_test_dataset(self):\n",
    "        if not self.test_dataset_path.exists():\n",
    "            raise FileNotFoundError(f\"Test dataset not found: {self.test_dataset_path}\")\n",
    "        df = pd.read_csv(self.test_dataset_path)\n",
    "        # 校验字段\n",
    "        must_columns = ['user_input','reference_contexts','reference']\n",
    "        for col in must_columns:\n",
    "            if col not in df.columns:\n",
    "                raise ValueError(f\"Missing column: {col}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3f9399",
   "metadata": {},
   "source": [
    "#### 生成RAG响应"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02ce696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag import default_rag_client\n",
    "import asyncio\n",
    "async def gen_rag_response(self):\n",
    "        rag_logs_dir = self.results_dir / \"logs\"\n",
    "        rag_logs_dir.mkdir(exist_ok=True)\n",
    "        rag_client = default_rag_client(llm_client=self.chat_model, logdir=str(rag_logs_dir))\n",
    "\n",
    "        top_k = 5\n",
    "\n",
    "        async def run_case(row):\n",
    "            question = row[\"user_input\"]\n",
    "            last_error = None\n",
    "            for attempt in range(self.eval_max_retries):\n",
    "                try:\n",
    "                    result = await asyncio.wait_for(\n",
    "                        rag_client.query(question, top_k=top_k),\n",
    "                        timeout=self.eval_timeout\n",
    "                    )\n",
    "                    answer = result.get(\"answer\", \"\")\n",
    "                    # 如果返回的是错误信息则重试\n",
    "                    if isinstance(answer, str) and answer.startswith(\"Error\"):\n",
    "                        last_error = answer\n",
    "                        continue\n",
    "                    return {\n",
    "                        \"user_input\": question,\n",
    "                        \"reference_contexts\": row.get(\"reference_contexts\", \"\"),\n",
    "                        \"reference\": row.get(\"reference\", \"\"),\n",
    "                        \"answer\": answer,\n",
    "                        \"run_id\": result.get(\"run_id\", \"\"),\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    last_error = str(e)\n",
    "                    continue\n",
    "            # 重试失败后返回错误\n",
    "            return {\n",
    "                \"user_input\": question,\n",
    "                \"reference_contexts\": row.get(\"reference_contexts\", \"\"),\n",
    "                \"reference\": row.get(\"reference\", \"\"),\n",
    "                \"answer\": f\"Error after retries: {last_error}\",\n",
    "                \"run_id\": \"\",\n",
    "            }\n",
    "\n",
    "        tasks = [run_case(row) for _, row in self.test_cases.iterrows()]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        df = pd.DataFrame(results)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748dc8b0",
   "metadata": {},
   "source": [
    "#### 评估RAG生成质量\n",
    "- eval_rag_response 评估RAG生成质量的主函数\n",
    "- eval_single_response 评估单个RAG生成的答案\n",
    "- worker 异步任务函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c82ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "async def eval_rag_response(self,eval_dataset:pd.DataFrame):\n",
    "        must_columns = ['user_input','reference_contexts','reference','answer']\n",
    "        for col in must_columns:\n",
    "            if col not in eval_dataset.columns:\n",
    "                raise ValueError(f\"Missing column: {col}\")\n",
    "        \n",
    "        # 准备数据，转换为 Ragas Dataset 格式\n",
    "        data = {\n",
    "            \"question\": eval_dataset[\"user_input\"].tolist(),\n",
    "            \"answer\": eval_dataset[\"answer\"].tolist(),\n",
    "            \"ground_truth\": eval_dataset[\"reference\"].tolist(),\n",
    "            \"retrieved_contexts\": [],\n",
    "        }\n",
    "        \n",
    "        for _, row in eval_dataset.iterrows():\n",
    "            ctx = row[\"reference_contexts\"]\n",
    "            if isinstance(ctx, str):\n",
    "                data[\"retrieved_contexts\"].append([ctx])\n",
    "            elif isinstance(ctx, list):\n",
    "                data[\"retrieved_contexts\"].append(ctx)\n",
    "            else:\n",
    "                data[\"retrieved_contexts\"].append([str(ctx)] if ctx is not None else [])\n",
    "\n",
    "        dataset = Dataset.from_dict(data)\n",
    "\n",
    "        metrics = [\n",
    "            ContextPrecision(), \n",
    "            ContextRecall(), \n",
    "            AnswerRelevancy(), \n",
    "            ContextEntityRecall(), \n",
    "            NoiseSensitivity(), \n",
    "            Faithfulness() \n",
    "        ]\n",
    "        \n",
    "        logger.info(\"Starting bulk evaluation with Ragas...\")\n",
    "        \n",
    "        # 使用 asyncio.to_thread 避免阻塞主事件循环，因为 evaluate 可能是同步调用的\n",
    "        result = await asyncio.to_thread(\n",
    "            evaluate,\n",
    "            dataset=dataset,\n",
    "            metrics=metrics,\n",
    "            llm=self.llm,\n",
    "            embeddings=self.embeddings,\n",
    "            raise_exceptions=False\n",
    "        )\n",
    "        \n",
    "        out_df = result.to_pandas()\n",
    "        \n",
    "        outfile = self.results_dir / f\"rag_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        out_df.to_csv(outfile, index=False, encoding=\"utf-8\")\n",
    "        logger.info(\"Saved RAG eval results to %s\", outfile)\n",
    "        return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52bbb1c",
   "metadata": {},
   "source": [
    "#### 结果展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb390b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def _display_results_table(self, results: pd.DataFrame):\n",
    "        logger.info(\"\")\n",
    "        logger.info(\"%s\", \"=\" * 115)\n",
    "        logger.info(\"📊 EVALUATION RESULTS SUMMARY\")\n",
    "        logger.info(\"%s\", \"=\" * 115)\n",
    "\n",
    "        logger.info(\n",
    "            \"%-4s | %-50s | %6s | %7s | %6s | %7s | %6s | %6s | %6s | %6s\",\n",
    "            \"#\",\n",
    "            \"Question\",\n",
    "            \"Faith\",\n",
    "            \"AnswRel\",\n",
    "            \"CtxRec\",\n",
    "            \"CtxPrec\",\n",
    "            \"EntRec\",\n",
    "            \"Noise\",\n",
    "            \"RAGAS\",\n",
    "            \"Status\",\n",
    "        )\n",
    "        logger.info(\"%s\", \"-\" * 115)\n",
    "\n",
    "        def fmt(val, width):\n",
    "            try:\n",
    "                if val is None or (isinstance(val, float) and (pd.isna(val) or val != val)) or (hasattr(np, \"isnan\") and np.isnan(val)):\n",
    "                    return f\"{'N/A':>{width}}\"\n",
    "                return f\"{float(val):>{width}.3f}\"\n",
    "            except Exception:\n",
    "                return f\"{str(val)[:width]:>{width}}\"\n",
    "\n",
    "        for idx, row in results.iterrows():\n",
    "            q = row.get(\"question\", row.get(\"user_input\", \"\"))\n",
    "            q_disp = (q[:47] + \"...\") if isinstance(q, str) and len(q) > 50 else q\n",
    "            is_error = row.get(\"status\", \"\") == \"error\"\n",
    "            if is_error:\n",
    "                err = row.get(\"error\", \"Unknown error\")\n",
    "                err_disp = (err[:20] + \"...\") if isinstance(err, str) and len(err) > 23 else err\n",
    "                logger.info(\n",
    "                    \"%-4d | %-50s | %6s | %7s | %6s | %7s | %6s | %6s | %6s | ✗ %s\",\n",
    "                    idx + 1,\n",
    "                    q_disp,\n",
    "                    \"N/A\",\n",
    "                    \"N/A\",\n",
    "                    \"N/A\",\n",
    "                    \"N/A\",\n",
    "                    \"N/A\",\n",
    "                    \"N/A\",\n",
    "                    \"N/A\",\n",
    "                    err_disp,\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            faith = row.get(\"faithfulness\", None)\n",
    "            ans_rel = row.get(\"answer_relevancy\", row.get(\"answer_relevance\", None))\n",
    "            ctx_rec = row.get(\"context_recall\", None)\n",
    "            ctx_prec = row.get(\"context_precision\", None)\n",
    "            ent_rec = row.get(\"context_entity_recall\", None)\n",
    "            noise = row.get(\"noise_sensitivity\", None)\n",
    "            ragas = row.get(\"ragas_score\", None)\n",
    "            logger.info(\n",
    "                \"%-4d | %-50s | %s | %s | %s | %s | %s | %s | %6s | %s\",\n",
    "                idx + 1,\n",
    "                q_disp,\n",
    "                fmt(faith, 6),\n",
    "                fmt(ans_rel, 7),\n",
    "                fmt(ctx_rec, 6),\n",
    "                fmt(ctx_prec, 7),\n",
    "                fmt(ent_rec, 6),\n",
    "                fmt(noise, 6),\n",
    "                fmt(ragas, 6),\n",
    "                \"✓\",\n",
    "            )\n",
    "\n",
    "        logger.info(\"%s\", \"=\" * 115)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hello-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
